{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D-Gap surrogate modeling by means of Multi-fidelity Gaussian Process Regression\n",
    "\n",
    "This notebook covers the procedure of how to use multiple fidelities in order to obtain a better Gaussian process surrogate model with less computational cost. The setup assumes the 2D-Pad gap size problem with two features $h_0$ and $r_s$ as introduced in Notebook 1.\n",
    "\n",
    "In order to get started, a modified version of `GPy` which accomodates multi-fidelity Gaussian processes has been downloaded and imported into this folder. Any updates to the code in the future is available on github and can be found [here](https://github.com/taylanot/GPy). In order to avoid confusion with the existing `GPy` framework (without multi-fidelity), <ins>the modified module containing the multi-fidelity code should be renamed</ins>. In this notebook, the name `GPy_MF` is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning in stationary: failed to import cython module: falling back to numpy\n",
      "warning in coregionalize: failed to import cython module: falling back to numpy\n",
      "warning in choleskies: failed to import cython module: falling back to numpy\n"
     ]
    }
   ],
   "source": [
    "import GPy_MF.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, some standard packages are imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the input data is set-up on which the surrogate model will be built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xc1 = np.linspace(50, 200, 10).reshape(-1,1) # Design parameter range for r_s\n",
    "Xc2 = np.linspace(30, 34.18, 10).reshape(-1,1) # Design parameter range for  h_0\n",
    "xxc, yyc = np.meshgrid(Xc1, Xc2) # For plotting\n",
    "\n",
    "# Design grid on which the Gaussian processes are to be predicted\n",
    "des_grid_x = np.linspace(50, 200, 100)\n",
    "des_grid_y = np.linspace(30, 34.18, 100)\n",
    "des_grid_xx, des_grid_yy = np.meshgrid(des_grid_x, des_grid_y)\n",
    "des_grid = np.array([des_grid_xx.reshape(-1, 1), des_grid_yy.reshape(-1, 1)]).squeeze().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the response data should be introduced. Please refer to Notebook 1 to see how this data is generated. To be precise, we have sampled the 2D response surfaces in a 10x10 grid in multiple different fidelity settings.\n",
    "\n",
    "In what follows, two fidelities are considered when performing the regression; one low fidelity and one high fidelity. The (reference) high-fidelity response surface is provided in `resp_data_HF.csv`, and an assortment of five different types of low fidelity sweeps can be found in `resp_data_LF1.csv` through `resp_data_LF5.csv`. All files are also equipped with the input data for `r_s` and `h_0`.\n",
    "\n",
    "Here is a table summarizing the models that generated the responses:\n",
    "\n",
    "| HF | LF1 | LF2 | LF3 | LF4 | LF5 |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| Original mesh & mass | Coarser mesh only | 10x mass scale only | 25x mass scale only | Coarser mesh & 10x mass scale | Coarser mesh & 25x mass scale |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_LF = pd.read_csv('Data/resp_data_LF5.csv')\n",
    "df_HF = pd.read_csv('Data/resp_data_HF.csv')\n",
    "\n",
    "Xc = np.array(df_LF[['rs', 'h0']]).reshape((100, 2))\n",
    "\n",
    "scaler = StandardScaler() # Data needs to be scaled for regression\n",
    "scaler.fit(Xc)\n",
    "Xc_scaled = scaler.transform(Xc)\n",
    "des_grid_scaled = scaler.transform(des_grid)\n",
    "\n",
    "CheapResp = np.array(df_LF['gap']).reshape((10, 10)) # Other low-fidelity response surfaces can be selected here.\n",
    "ExpensiveResp = np.array(df_HF['gap']).reshape((10, 10))\n",
    "\n",
    "Yc = CheapResp.reshape(-1, 1)\n",
    "Ye_full = ExpensiveResp.reshape(-1, 1)\n",
    "\n",
    "Expensive_DoE_indices = np.random.choice(100, 10, replace=False)\n",
    "\n",
    "Xe = Xc[Expensive_DoE_indices]\n",
    "Xe_scaled = Xc_scaled[Expensive_DoE_indices]\n",
    "\n",
    "Ye = Ye_full[Expensive_DoE_indices]\n",
    "\n",
    "### MFGPR method input setup\n",
    "\n",
    "x = des_grid_scaled\n",
    "\n",
    "Xl = Xc_scaled\n",
    "Xh = Xe_scaled\n",
    "\n",
    "X = [Xl, Xh]\n",
    "\n",
    "Yl = Yc\n",
    "Yh = Ye\n",
    "\n",
    "Y = [Yl, Yh]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the data has been prepared to perform the regression, we first construct a (single-fidelity) reference Gaussian process surrogate model for the 10x10 high-fidelity response surface. \n",
    "The radial basis function (RBF) kernel used for the GP regression has three hyper-parameters: `rbf.variance`, `rbf.lengthscale` and `Gaussian_noise.variance`. These hyperparameters are able to be optimized within `GPy` by means of maximizing the log marginal likelihood of the Gaussian process. Here, the (default) L-BFGS-B algorithm is used, and other algorithms are available. Please refer to the [documentation of `GPy`](https://gpy.readthedocs.io/en/deploy/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mExp = GPy_MF.models.GPRegression(Xl, Ye_full) # Full expensive GPR surrogate\n",
    "\n",
    "### Uncomment any of the following lines to fix a certain hyperparameter to a predetermined value. ###\n",
    "# mExp.parameters[0]['rbf.variance'].fix(0.5)\n",
    "# mExp.parameters[0]['rbf.lengthscale'].fix(0.5)\n",
    "# mExp.parameters[1]['Gaussian_noise.variance'].fix(0.5)\n",
    "\n",
    "mExp.optimize(max_iters=4)\n",
    "\n",
    "muExp, sigmaExp = mExp.predict(x)\n",
    "\n",
    "mExp_lim = GPy_MF.models.GPRegression(Xh, Ye) # Restricted expensive GPR surrogate\n",
    "mExp_lim.optimize(max_iters=4)\n",
    "muExp_lim, sigmaExp_lim = mExp_lim.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-fidelity GPR is performed below. Since the same kernel is used, we are dealing with the same hyper-parameters (for each fidelity) and they are optimized in the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = GPy_MF.models.multiGPRegression(X, Y) # Multi-fidelity GPR surrogate\n",
    "\n",
    "### Uncomment any of the following lines to fix a certain hyperparameter to a predetermined value. ###\n",
    "# m.models[0]['Gaussian_noise.variance'].fix(0.1)\n",
    "# m.models[0]['rbf.variance'].fix(1)\n",
    "# m.models[0]['rbf.lengthscale'].fix(1)\n",
    "# m.models[1]['Gaussian_noise.variance'].fix(0.05)\n",
    "# m.models[1]['rbf.variance'].fix(1)\n",
    "# m.models[1]['rbf.lengthscale'].fix(1)\n",
    "\n",
    "m.optimize_restarts(restarts=4, verbose=False)\n",
    "\n",
    "mu_mf, sigma_mf = m.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can output and analyze the hyperparameters and $R^2$-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (MF)GPR hyperparameters\n",
    "print('Single-fidelity reference GPR:')\n",
    "print(mExp)\n",
    "print('--------------------------------')\n",
    "\n",
    "print('Multi-fidelity GPR:')\n",
    "print(m)\n",
    "print('--------------------------------')\n",
    "\n",
    "# Coefficients of determination\n",
    "print('R-squared score of full expensive GPR surface compared with...')\n",
    "print('Cheap GPR surface:', r2_score(muExp, mu_mf[0]))\n",
    "print('Expensive GPR surface:', r2_score(muExp, muExp_lim))\n",
    "print('Multi-fidelity GPR surface:', r2_score(muExp, mu_mf[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is evident from the final two lines from the previous cell, the multi-fidelity Gaussian process surrogate model is (much) more accurate than when either just the low- or high-fidelity FE model output is considered.\n",
    "\n",
    "This last code cell is meant to visualize the computed results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = [a.reshape(np.shape(des_grid_xx)) for a in mu_mf]\n",
    "sigma = [a.reshape(np.shape(des_grid_xx)) for a in sigma_mf]\n",
    "\n",
    "### Visualization\n",
    "\n",
    "fig1, axs1 = plt.subplots(2, 3, figsize=(8, 5))\n",
    "\n",
    "axs1[0, 0].contourf(xxc, yyc, CheapResp.reshape(np.shape(xxc)))\n",
    "axs1[0, 0].set_title('Cheap response surface')\n",
    "\n",
    "axs1[1, 0].contourf(xxc, yyc, ExpensiveResp.reshape(np.shape(xxc)))\n",
    "axs1[1, 0].set_title('Expensive response surface')\n",
    "\n",
    "axs1[0, 1].contourf(des_grid_xx, des_grid_yy, mu[0])\n",
    "axs1[0, 1].set_title('Low-fidelity GPR surface')\n",
    "\n",
    "axs1[1, 1].contourf(des_grid_xx, des_grid_yy, mu[1])\n",
    "axs1[1, 1].set_title('Multi-fidelity GPR surface')\n",
    "\n",
    "axs1[0, 2].contourf(des_grid_xx, des_grid_yy, sigma[0])\n",
    "axs1[0, 2].set_title('Low-fidelity variance')\n",
    "\n",
    "axs1[1, 2].contourf(des_grid_xx, des_grid_yy, sigma[1])\n",
    "axs1[1, 2].set_title('Multi-fidelity variance')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig2 = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax1 = fig2.add_subplot(2, 2, 1, projection='3d')\n",
    "ax1.view_init(20, 225)\n",
    "ax1.plot_surface(xxc, yyc, CheapResp.reshape(np.shape(xxc)), cmap='viridis')\n",
    "ax1.set_title('Cheap response surface')\n",
    "\n",
    "ax2 = fig2.add_subplot(2, 2, 2, projection='3d')\n",
    "ax2.view_init(20, 225)\n",
    "ax2.plot_surface(des_grid_xx, des_grid_yy, mu[0], cmap='viridis')\n",
    "ax2.set_title('Cheap GPR surface')\n",
    "\n",
    "ax3 = fig2.add_subplot(2, 2, 3, projection='3d')\n",
    "ax3.view_init(20, 225)\n",
    "ax3.plot_surface(xxc, yyc, ExpensiveResp.reshape(np.shape(xxc)), cmap='viridis')\n",
    "ax3.set_title('Expensive response surface')\n",
    "\n",
    "ax4 = fig2.add_subplot(2, 2, 4, projection='3d')\n",
    "ax4.view_init(20, 225)\n",
    "ax4.plot_surface(des_grid_xx, des_grid_yy, mu[1], cmap='viridis')\n",
    "ax4.set_title('MFGPR surface')\n",
    "\n",
    "fig3 = plt.figure()\n",
    "axEx = fig3.add_subplot(1, 1, 1, projection='3d')\n",
    "axEx.view_init(20, 225)\n",
    "axEx.plot_surface(des_grid_xx, des_grid_yy, muExp.reshape(np.shape(des_grid_xx)), cmap='viridis')\n",
    "axEx.set_title('Full GPR surface')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
